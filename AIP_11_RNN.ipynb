{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TDOHLpfldh2L"
      },
      "source": [
        "**Sentiment Analysis using RNN models**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YA-1VojEdabb"
      },
      "outputs": [],
      "source": [
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords \n",
        "from collections import Counter\n",
        "import string\n",
        "import re\n",
        "from tqdm import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "from sklearn.model_selection import train_test_split"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A_5UtRWzwOq8"
      },
      "source": [
        "**Dataset loading**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cDHzi04bdl2P"
      },
      "outputs": [],
      "source": [
        "#구글 드라이브 마운트\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "#zip 파일 현재 폴더로 복사, 중간 부분은 파일명에 따라 변경 필요\n",
        "!cp '/content/drive/MyDrive/IMDB Dataset.csv.zip' ./"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U9OMOr-vmXiT"
      },
      "outputs": [],
      "source": [
        "#압축 해제\n",
        "!unzip 'IMDB Dataset.csv.zip'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vXiBe_7dmqZp"
      },
      "outputs": [],
      "source": [
        "is_cuda = torch.cuda.is_available()\n",
        "\n",
        "# If we have a GPU available, we'll set our device to GPU. We'll use this device variable later in our code.\n",
        "if is_cuda:\n",
        "    device = torch.device(\"cuda\")\n",
        "    print(\"GPU is available\")\n",
        "else:\n",
        "    device = torch.device(\"cpu\")\n",
        "    print(\"GPU not available, CPU used\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kF_-x0R9mw_q"
      },
      "outputs": [],
      "source": [
        "#구현 필요"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JtPf72_6wTlm"
      },
      "source": [
        "**Pre-process the dataset**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_M7nWpEsm1Aj"
      },
      "outputs": [],
      "source": [
        "#구현 필요\n",
        "\n",
        "print(f'shape of train data is {x_train.shape}')\n",
        "print(f'shape of test data is {x_test.shape}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R2PtZ0hUnDgS"
      },
      "outputs": [],
      "source": [
        "def preprocess_string(s):\n",
        "    # Remove all non-word characters (everything except numbers and letters)\n",
        "    s = re.sub(r\"[^\\w\\s]\", '', s)\n",
        "    # Replace all runs of whitespaces with no space\n",
        "    s = re.sub(r\"\\s+\", '', s)\n",
        "    # replace digits with no space\n",
        "    s = re.sub(r\"\\d\", '', s)\n",
        "\n",
        "    return s\n",
        "\n",
        "def tockenize(x_train,y_train,x_val,y_val):\n",
        "    word_list = []\n",
        "\n",
        "    stop_words = set(stopwords.words('english')) \n",
        "    for sent in x_train:\n",
        "        for word in sent.lower().split():\n",
        "            word = preprocess_string(word)\n",
        "            if word not in stop_words and word != '':\n",
        "                word_list.append(word)\n",
        "  \n",
        "    corpus = Counter(word_list)\n",
        "    # sorting on the basis of most common words\n",
        "    corpus_ = sorted(corpus,key=corpus.get,reverse=True)[:1000]\n",
        "    # creating a dict\n",
        "    onehot_dict = {w:i+1 for i,w in enumerate(corpus_)}\n",
        "    \n",
        "    # tockenize\n",
        "    final_list_train,final_list_test = [],[]\n",
        "    for sent in x_train:\n",
        "            final_list_train.append([onehot_dict[preprocess_string(word)] for word in sent.lower().split() \n",
        "                                     if preprocess_string(word) in onehot_dict.keys()])\n",
        "    for sent in x_val:\n",
        "            final_list_test.append([onehot_dict[preprocess_string(word)] for word in sent.lower().split() \n",
        "                                    if preprocess_string(word) in onehot_dict.keys()])\n",
        "            \n",
        "    encoded_train = [1 if label =='positive' else 0 for label in y_train]  \n",
        "    encoded_test = [1 if label =='positive' else 0 for label in y_val] \n",
        "    return np.array(final_list_train), np.array(encoded_train),np.array(final_list_test), np.array(encoded_test),onehot_dict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_Ccbdr5IomDb"
      },
      "outputs": [],
      "source": [
        "#구현 필요\n",
        "\n",
        "print(f'Length of vocabulary is {len(vocab)}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NYv719ZFopKD"
      },
      "outputs": [],
      "source": [
        "rev_len = [len(i) for i in x_train]\n",
        "pd.Series(rev_len).hist()\n",
        "plt.show()\n",
        "pd.Series(rev_len).describe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jRyJW2jBpURd"
      },
      "outputs": [],
      "source": [
        "def padding_(sentences, seq_len):\n",
        "    features = np.zeros((len(sentences), seq_len),dtype=int)\n",
        "    for ii, review in enumerate(sentences):\n",
        "        if len(review) != 0:\n",
        "            features[ii, -len(review):] = np.array(review)[:seq_len]\n",
        "    return features\n",
        "\n",
        "#구현 필요"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HAlTx-GwpnEz"
      },
      "outputs": [],
      "source": [
        "#구현 필요\n",
        "\n",
        "# create Tensor datasets\n",
        "\n",
        "# dataloaders\n",
        "batch_size = 50\n",
        "\n",
        "# make sure to SHUFFLE your data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qKtc7tXGpxCS"
      },
      "outputs": [],
      "source": [
        "# obtain one batch of training data\n",
        "dataiter = iter(train_loader)\n",
        "sample_x, sample_y = next(dataiter)\n",
        "\n",
        "print('Sample input size: ', sample_x.size()) # batch_size, seq_length\n",
        "print('Sample input: \\n', sample_x)\n",
        "print('Sample input: \\n', sample_y)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t6KGQ05Twx2r"
      },
      "source": [
        "**GRU model code**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FwfDenLCp01j"
      },
      "outputs": [],
      "source": [
        "class GRU_model(nn.Module):\n",
        "    def __init__(self, n_layers, hidden_dim, n_vocab, embed_dim, n_classes, device):\n",
        "        super(GRU_model, self).__init__()\n",
        "        self.n_layers = n_layers\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.device = device\n",
        "\n",
        "        #구현 필요\n",
        "\n",
        "    def forward(self, x):\n",
        "        #구현 필요\n",
        "        \n",
        "        # 첫번째 히든 스테이트를 0벡터로 초기화\n",
        "        # GRU의 리턴값은 (배치 크기, 시퀀스 길이, 은닉 상태의 크기)\n",
        "        # (배치 크기, 은닉 상태의 크기)의 텐서로 크기가 변경됨. 즉, 마지막 time-step의 은닉 상태만 가져온다.\n",
        "        # (배치 크기, 은닉 상태의 크기) -> (배치 크기, 출력층의 크기)\n",
        "        return logit\n",
        "\n",
        "    def _init_state(self, batch_size):\n",
        "        #구현 필요\n",
        "        \n",
        "        return new_state"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rISdhnCDrYr1"
      },
      "outputs": [],
      "source": [
        "n_layers = 1\n",
        "vocab_size = len(vocab) + 1  # extra 1 for <pad>\n",
        "hidden_dim = 128\n",
        "n_classes = 2\n",
        "\n",
        "#구현 필요"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V02dcuLjw2hV"
      },
      "source": [
        "**Train and evaluation**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i-CcnIS7s8H7"
      },
      "outputs": [],
      "source": [
        "def train(model, criterion, optimizer, data_loader):\n",
        "    model.train()\n",
        "    train_loss = 0\n",
        "    for i, (x, y) in enumerate(data_loader):\n",
        "        x, y = x.to(device), y.to(device)\n",
        "        \n",
        "        optimizer.zero_grad()\n",
        "        logit = model(x)\n",
        "        loss = criterion(logit, y)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        train_loss += loss.item() * x.size(0)\n",
        "      \n",
        "    return train_loss / len(data_loader.dataset)\n",
        "\n",
        "def evaluate(model, data_loader):\n",
        "    model.eval()\n",
        "    corrects, total_loss = 0, 0\n",
        "    for i, (x, y) in enumerate(data_loader):\n",
        "        x, y = x.to(device), y.to(device)\n",
        "\n",
        "        logit = model(x)\n",
        "        corrects += (logit.max(1)[1].view(y.size()).data == y.data).sum()\n",
        "    size = len(data_loader.dataset)\n",
        "    \n",
        "    avg_accuracy = 100.0 * corrects / size\n",
        "    return avg_accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LFdNDdr5sBXd"
      },
      "outputs": [],
      "source": [
        "num_epochs = 10\n",
        "lr = 0.001\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "\n",
        "for e in range(1, num_epochs+1):\n",
        "    #구현 필요\n",
        "\n",
        "    print(\"[Epoch: %d] train loss : %5.2f | test accuracy : %5.2f\" % (e, train_loss, test_accuracy))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gko6X5-Yw-XM"
      },
      "source": [
        "**LSTM model code**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GnvuLbEKvRaO"
      },
      "outputs": [],
      "source": [
        "class LSTM_model(nn.Module):\n",
        "    def __init__(self, n_layers, hidden_dim, n_vocab, embed_dim, n_classes, device):\n",
        "        super(LSTM_model, self).__init__()\n",
        "        self.n_layers = n_layers\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.device = device\n",
        "\n",
        "        #구현 필요\n",
        "\n",
        "    def forward(self, x):\n",
        "        #구현 필요\n",
        "        \n",
        "        # 첫번째 히든 스테이트를 0벡터로 초기화\n",
        "        # LSTM의 리턴값 또한 (배치 크기, 시퀀스 길이, 은닉 상태의 크기)\n",
        "        # (배치 크기, 은닉 상태의 크기)의 텐서로 크기가 변경됨. 즉, 마지막 time-step의 은닉 상태만 가져온다.\n",
        "        # (배치 크기, 은닉 상태의 크기) -> (배치 크기, 출력층의 크기)\n",
        "        return logit\n",
        "\n",
        "    def _init_state(self, batch_size):\n",
        "        #구현 필요\n",
        "        return (new_hidden_state, new_cell_state)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bt0oCIHKxryU"
      },
      "source": [
        "**LSTM training and evaluation**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QadaToSExn_-"
      },
      "outputs": [],
      "source": [
        "#구현 필요\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "for e in range(1, num_epochs+1):\n",
        "    #구현 필요\n",
        "\n",
        "    print(\"[Epoch: %d] train loss : %5.2f | test accuracy : %5.2f\" % (e, train_loss, test_accuracy))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KiJVBRbsyVWO"
      },
      "source": [
        "**Vanilla RNN model code**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NtHx26FZxxyU"
      },
      "outputs": [],
      "source": [
        "class RNN_model(nn.Module):\n",
        "    def __init__(self, n_layers, hidden_dim, n_vocab, embed_dim, n_classes, device):\n",
        "        super(RNN_model, self).__init__()\n",
        "        self.n_layers = n_layers\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.device = device\n",
        "\n",
        "        #구현 필요\n",
        "\n",
        "    def forward(self, x):\n",
        "        #구현 필요\n",
        "\n",
        "        # 첫번째 히든 스테이트를 0벡터로 초기화\n",
        "        # RNN의 리턴값은 (배치 크기, 시퀀스 길이, 은닉 상태의 크기)\n",
        "        # (배치 크기, 은닉 상태의 크기)의 텐서로 크기가 변경됨. 즉, 마지막 time-step의 은닉 상태만 가져온다.\n",
        "        # (배치 크기, 은닉 상태의 크기) -> (배치 크기, 출력층의 크기)\n",
        "        return logit\n",
        "\n",
        "    def _init_state(self, batch_size):\n",
        "        #구현 필요\n",
        "        return new_state"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vsOZFxmwygdV"
      },
      "outputs": [],
      "source": [
        "#구현 필요\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "for e in range(1, num_epochs+1):\n",
        "    #구현 필요\n",
        "\n",
        "    print(\"[Epoch: %d] train loss : %5.2f | test accuracy : %5.2f\" % (e, train_loss, test_accuracy))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1JnTb4N5ylAF"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3.10.2 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.2"
    },
    "vscode": {
      "interpreter": {
        "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
