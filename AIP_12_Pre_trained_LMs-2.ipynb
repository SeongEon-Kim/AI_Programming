{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TDOHLpfldh2L"
   },
   "source": [
    "**IMDB sentiment analysis using DistillBERT**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UVn-1h83vrYt"
   },
   "outputs": [],
   "source": [
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-j8lrL2XKjyA"
   },
   "outputs": [],
   "source": [
    "!wget http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n",
    "!tar -xf aclImdb_v1.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IMReeT3On7Bk"
   },
   "outputs": [],
   "source": [
    "# 텍스트 형태의 데이터를 data, label 각각을 관리하기 위한 리스트 형태로 변환\n",
    "# train data, label & test data, label의 25,000개 데이터로 나뉨\n",
    "from pathlib import Path\n",
    "\n",
    "def read_imdb_split(split_dir):\n",
    "    split_dir = Path(split_dir)\n",
    "    texts = []\n",
    "    labels = []\n",
    "    for label_dir in [\"pos\", \"neg\"]:\n",
    "        for text_file in (split_dir/label_dir).iterdir():\n",
    "            texts.append(text_file.read_text())\n",
    "            labels.append(0 if label_dir is \"neg\" else 1)\n",
    "\n",
    "    return texts, labels\n",
    "\n",
    "#구현 필요\n",
    "train_texts, train_labels = read_imdb_split('acIImdb/train')\n",
    "test_texts, test_labels = read_imdb_split('acIImdb/test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "e0i_hZbAoKMp"
   },
   "outputs": [],
   "source": [
    "# 문장 토큰화를 위해 tokenizer로드\n",
    "# DistillBERT를 학습시킬 때 사용한 tokenizer를 활용해야 pre-trained LM을 로드한 후 학습할 때 원하는 대로 임베딩 값이 입력됨\n",
    "# WordPiece라는 토큰화 및 임베딩 방식을 활용함 (단어를 더 쪼개서 sub-word 토큰을 만듦)\n",
    "# 워드 임베딩에 비해 적은 수의 토큰으로 ㅁ낳은 단어를 포현할 수 있고, 처음 보는 단어도 토큰을 조합해서 임베딩을 나타낼 수 있게 됨\n",
    "from transformers import DistilBertTokenizerFast\n",
    "#구현 필요\n",
    "tokenizer= DistilBertTokenizerFast.from_pretrained('distilbert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AYUbsq9hoTWT"
   },
   "outputs": [],
   "source": [
    "# 로딩된 tokenizer 활용하여 토큰화 진행\n",
    "# truncation 및 padding은 max_length 길이에 맞지 않을 때 자르거나 padding을 붙이는 것에 대한 파라미터를 의미함\n",
    "#구현 필요\n",
    "train_encodings = tokenizer(train_texts, truncation=True, padding=True)\n",
    "test_encodings = tokenizer(test_texts, truncation=True, padding=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "764l_RZyoVLW"
   },
   "outputs": [],
   "source": [
    "# 모델 입력을 위한 Pytorch 데이터셋 생성\n",
    "import torch\n",
    "\n",
    "class IMDbDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "#구현 필요\n",
    "train_dataset =  IMDbDataset(test_encodings, train_labels)\n",
    "test_dataset = IMDbDataset(test_encodings, test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TmNj7Ne7oXMm"
   },
   "outputs": [],
   "source": [
    "# 학습을 위한 각종 하이퍼파라미터 설정 및 pre-trained LM 로딩\n",
    "\n",
    "from transformers import DistilBertForSequenceClassification, Trainer, TrainingArguments\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',          # output directory\n",
    "    num_train_epochs=1,              # total number of training epochs\n",
    "    per_device_train_batch_size=25,  # batch size per device during training\n",
    "    per_device_eval_batch_size=50,   # batch size for evaluation\n",
    "    warmup_steps=100,                # number of warmup steps for learning rate scheduler\n",
    "    weight_decay=0.01,               # strength of weight decay\n",
    "    logging_dir='./logs',            # directory for storing logs\n",
    "    logging_steps=250,\n",
    ")\n",
    "\n",
    "#구현 필요\n",
    "model = DistilBertForSequenceClassification.from_pretrained(\"distilbert-base-uncased\")\n",
    "\n",
    "# Transformers 라이브러리의 Trainer 객체를 이용해 편리하게 학습 진행 가능\n",
    "def compute_metrics(pred):    # 기본적으로 Trainer에서 evaluate 메서드를 호출하면 loss 및 예측 속도 측면에서의 성능만 기록됨\n",
    "    labels = pred.label_ids     # 이에 추가적인 성능 지표를 사용하고 싶다면 좌측과 같은 형식을 갖춘 함수를 정의 후 trainer 객체 생성 시 인자로 넣어주어야 함\n",
    "    preds = pred.predictions.argmax(-1)\n",
    "    \n",
    "    acc = accuracy_score(labels, preds)    \n",
    "    return {\n",
    "        'accuracy': acc\n",
    "    }\n",
    "\n",
    "#구현 필요\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    "    compute_metrics=compute_metrics # 없으면 정확도 출력 x only loss만 출력\n",
    ")\n",
    "\n",
    "# 테스트 결과 93% 정도의 정확도를 기록함\n",
    "# 지난 주 실습보다 훨씬 적은 개수의 학습 데이터를 사용하였지만 pre_trained LM의 효과로 훨씬 높은 정확도를 달성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WVjV7tbxoj_c"
   },
   "outputs": [],
   "source": [
    "# 테스트 함수\n",
    "metrics = trainer.evaluate(eval_dataset=test_dataset)\n",
    "#구현 필요"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PCm5P0yRokay"
   },
   "outputs": [],
   "source": [
    "metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HgGLyzoCsiWf"
   },
   "source": [
    "**Sentiment analysis**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "**transformer에서는 pipeline 이라는 high-level API를 통해 손쉽게 이미 학습되어있는 모델에 기반한 다양한 NLP task를 수행할 수 있음**\n",
    "**Pipeline에서는 로드하여 사용하는 모델이 있으나 객체 내부적으로 알아서 진행하는 것이며 직접 모델을 PyTorch로 불러와서 활용하는 방법도 가능함**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "43ntCok7sk2Y"
   },
   "outputs": [],
   "source": [
    "# pipeline을 호출할 대 현재 목표로 하는 task를 입력하면 관련 모델을 로드하여 곧바로 활용할 수 있도록 객체를 만들어 줌\n",
    "from transformers import pipeline\n",
    "\n",
    "#구현 필요\n",
    "nlp = pipeline(\"sentiment-analysis\")\n",
    "\n",
    "#구현 필요\n",
    "result = nlp(\"I hate you\")[0]\n",
    "print(f\"label: {result['label']}, with score: {round(result['score'], 4)}\")\n",
    "\n",
    "#구현 필요\n",
    "result = nlp(\"I Love you\")[0]\n",
    "print(f\"label: {result['label']}, with score: {round(result['score'], 4)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1RR8s7qe0LtX"
   },
   "source": [
    "**Question answering**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "**Question answering이란 지문과 문제를 풀고 단답형 주관식에 대한 답을 내는 task를 말함**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pgr8pYXhslWt"
   },
   "outputs": [],
   "source": [
    "# pipeline 생성 시 question-answering이라는 task를 입력하면 되고, question과 context 정보를 주면 답을 반환해 줌\n",
    "from transformers import pipeline\n",
    "\n",
    "#구현 필요\n",
    "nlp = pipeline(\"question-answering\")\n",
    "\n",
    "context = r\"\"\"\n",
    "Extractive Question Answering is the task of extracting an answer from a text given a question. An example of a\n",
    "question answering dataset is the SQuAD dataset, which is entirely based on that task. If you would like to fine-tune\n",
    "a model on a SQuAD task, you may leverage the examples/question-answering/run_squad.py script.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0Rw8VL_WwlfH"
   },
   "outputs": [],
   "source": [
    "#구현 필요\n",
    "result = nlp(question=\"what is extractive question answering?\", context=context)\n",
    "print(f\"Answer: '{result['answer']}', score: {round(result['score'], 4)}, start: {result['start']}, end: {result['end']}\")\n",
    "\n",
    "#구현 필요\n",
    "result = nlp(question=\"what is a good example of a question answering dataset?\", context=context)\n",
    "print(f\"Answer: '{result['answer']}', score: {round(result['score'], 4)}, start: {result['start']}, end: {result['end']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PRWrOjLd0OoR"
   },
   "source": [
    "**Text generation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "neRbA7ddw6ye"
   },
   "outputs": [],
   "source": [
    "# pipeline 생성 시 text-generation이라는 task를 입력하면 문장 생성 task를 수행가능함\n",
    "# 첫 단어 몇개를 나열해주면 그 뒤를 알아서 생성\n",
    "# max_length: 생성할 문장의 최대 단어 수\n",
    "# do_sample: 매 단어 생성시 top k개 중 샘플링 할 것인지에 대한 매개변수를 의미함\n",
    "\n",
    "from transformers import pipeline\n",
    "\n",
    "#구현 필요\n",
    "text_generator = pipeline(\"text-generation\")\n",
    "print(text_generator(\"As far as I am concerned, I will\", max_length=50, do_sample=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tCiAE5BOzaxw"
   },
   "outputs": [],
   "source": [
    "# do_sample을 설정하고 몇개의 상위 확률 단어들 중에 고를 것인지 top_k로 명시해주면 다른 생성 결과를 볼 수 있다\n",
    "\n",
    "#구현 필요\n",
    "print(text_generator(\"As far as I am concerned, I will\", max_length=50, do_sample=True, top_k=3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LZ3iEl4l0Q45"
   },
   "source": [
    "**Using the specific model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Pp_Va9hcxu23"
   },
   "outputs": [],
   "source": [
    "# text generation pipeline을 해당 모델을 활용해서 만들 수 있음\n",
    "# 모든 모델이 pipeline 기능이 지원되는 것은 아니며, pipeline 지원이 안되는 모델은 해당 모델 github 페이지 등을 참조하여 모델을 직접 활용하여야\n",
    "#구현 필요\n",
    "text_generator = pipeline(\"text-generation\", model=\"skt/kogpt2-base-v2\")\n",
    "print(text_generator(\"요즘 부쩍 날씨가 추워졌습니다\", max_length=50, do_sample =False))"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3.10.2 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.2"
  },
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
