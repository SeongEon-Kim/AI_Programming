{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**RNN을 이용한 네이버 영화 리뷰 감성 분석**\n",
    "//2~7은 NLP에서 필요한 과정\n",
    "1. 데이터 로드\n",
    "2. 중복 데이터 제거\n",
    "3. 정규식 이용한 한글 및 공백 이외 문자 제거\n",
    "4. Null 데이터 제거\n",
    "5. 형태소 분석기를 활용한 토큰화 및 불용어 제거 -> 한국어 NLP 특화된 처리가 필요한 과정\n",
    "6. Vocab 생성 및 단어 인덱스 형태의 데이터셋 생성\n",
    "7. 데이터에 대한 padding 진행\n",
    "8. PyTorch 데이터셋 생성\n",
    "9. 모델 정의, 학습 및 테스트 함수 정의;\n",
    "10. 학습 및 테스트 진행\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "**자연언어 종류 3가지**\n",
    "**고립어: 한자를 사용하는 중국어와 같이 단어의 형태가 시제, 인칭 변화 등에 따라 바뀌지 않고 고정된 형태의 언어**\n",
    "**굴절어: 영어, 독일어 등과 같이 시제, 인칭 벼화 등에 따라 단어의 형태가 변하는 언어 (띄어쓰기로 split해도 높은 퀄리티의 토큰 생성)**\n",
    "**교착어: 한국어, 일본어 등과 같이 단어에 어미가 붙어 시제, 인칭 등의 뜻이 최종 결정되는 언어**"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 한국어 텍스트 토큰화의 어려움\n",
    "**한국어 텍스트는 split을 수행하여 토큰화를 하였을 때, 조사가 붙어있어 서로 다른 단어로 인식**\n",
    "**한국어는 띄어쓰기 맞춤법이 있지만 띄어쓰기가 잘 지켜지지 않는 corpus도 많으므로 기본적인 split으로 만족스러운 토큰들을 얻기 어려움**\n",
    "### -> 한국어 텍스트에서 영어와 유사한 토큰들을 얻기 위해서는 형태소 단위의 분리기 필요함"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**형태소: 의미를 가진 가장 작은 단위의 말 (자립 형태소, 의존 형태소)**\n",
    "**형태소 분석기: 원문 텍스트에서 형태소를 추출하여 토큰화 해주는 모듈, 프로그램**\n",
    "**KoNLPy, 카카오의 khaii 등이 있음**"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**konlpy.tag에 대양한 형태소 분석기가 있다. 작동 방식과 세부 성능에 차이가 있다**\n",
    "**자주 쓰이는 분석기들**\n",
    "**Okt: Open Korean Text의 줄임말로써 예전에는 Twitter 분석기라고 불렸음. \"ㅋㅋ\", \"ㄴㄴ\"등과 같은 인터넷 용어들도 비교적 분석이 가능한 장점이 있음**\n",
    "**Kkma: 꼬꼬마 형탳소 분석기의 줄임말. 어미 분석을 자세하게 수행해준다는 것이 장점**\n",
    "**Mecab: 일본어 형태소 분석기 Mecab을 한국어 버전으로 구현한 것(+가장 무난하다)**"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "!pip install konlpy"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**자연언어는 세 종류로 구분됨**\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from konlpy.tag import Okt\n",
    "\n",
    "# KoNLPy 형태소 분석기들은 moprhs 메서드에 문장을 입력하여 형태소 분석이 가능\n",
    "# pos 메서드를 통해 형태소 분석 및 품사 태깅이 가능\n",
    "# 품사 태깅: 각 형태소마다 품사를 예측한 후 형태소-품사 pair을 만드는 것\n",
    "# 특정 품사는 제거하고 특정 품사는 남기는 등의 전처리 가능\n",
    "okt = Okt()\n",
    "print('Okt 형태소 분석 :', okt.morphs)(\"한국어 단어는 형태소들로 구성되어 있다.\")\n",
    "print('Okt 품사 태깅 :', okt.pos(\"한국어 단어는 형태소들로 구성되어 있다.\"))\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "erNa04VKS35K"
   },
   "outputs": [],
   "source": [
    "from konlpy.tag import Kkma\n",
    "kkma = Kkma()\n",
    "print(\"Kkma 형태소 분석 :\", kkma.morphs(\"힌극어 단어는 형태소들로 구성되어 있다.\"))\n",
    "print('Kkma 품사 태깅 :', kkma.pos(\"한국어 단어는 형태소들로 구성되어 있다.\"))\n",
    "\n",
    "#구현 필요"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jUacg-RwaK72"
   },
   "source": [
    "**네이버 영화 리뷰 감성분석**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "QrXjIS9iYR_g"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'konlpy'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mModuleNotFoundError\u001B[0m                       Traceback (most recent call last)",
      "\u001B[1;32m/Users/gimseong-eon/Downloads/AIP_13_NLP_for_Korean_Texts.ipynb 셀 6\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m      <a href='vscode-notebook-cell:/Users/gimseong-eon/Downloads/AIP_13_NLP_for_Korean_Texts.ipynb#W5sZmlsZQ%3D%3D?line=3'>4</a>\u001B[0m \u001B[39mimport\u001B[39;00m \u001B[39mre\u001B[39;00m\n\u001B[1;32m      <a href='vscode-notebook-cell:/Users/gimseong-eon/Downloads/AIP_13_NLP_for_Korean_Texts.ipynb#W5sZmlsZQ%3D%3D?line=4'>5</a>\u001B[0m \u001B[39mimport\u001B[39;00m \u001B[39murllib\u001B[39;00m\u001B[39m.\u001B[39;00m\u001B[39mrequest\u001B[39;00m\n\u001B[0;32m----> <a href='vscode-notebook-cell:/Users/gimseong-eon/Downloads/AIP_13_NLP_for_Korean_Texts.ipynb#W5sZmlsZQ%3D%3D?line=5'>6</a>\u001B[0m \u001B[39mfrom\u001B[39;00m \u001B[39mkonlpy\u001B[39;00m\u001B[39m.\u001B[39;00m\u001B[39mtag\u001B[39;00m \u001B[39mimport\u001B[39;00m Okt\n\u001B[1;32m      <a href='vscode-notebook-cell:/Users/gimseong-eon/Downloads/AIP_13_NLP_for_Korean_Texts.ipynb#W5sZmlsZQ%3D%3D?line=6'>7</a>\u001B[0m \u001B[39mfrom\u001B[39;00m \u001B[39mtqdm\u001B[39;00m \u001B[39mimport\u001B[39;00m tqdm\n\u001B[1;32m      <a href='vscode-notebook-cell:/Users/gimseong-eon/Downloads/AIP_13_NLP_for_Korean_Texts.ipynb#W5sZmlsZQ%3D%3D?line=7'>8</a>\u001B[0m \u001B[39mimport\u001B[39;00m \u001B[39mtorch\u001B[39;00m\n",
      "\u001B[0;31mModuleNotFoundError\u001B[0m: No module named 'konlpy'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import urllib.request\n",
    "from konlpy.tag import Okt\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from collections import Counter\n",
    "#device = torch.device('mps:0' if torch.backends.mps.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_2SWYgPFcSAD"
   },
   "outputs": [],
   "source": [
    "urllib.request.urlretrieve(\"https://raw.githubusercontent.com/e9t/nsmc/master/ratings_train.txt\", filename=\"ratings_train.txt\")\n",
    "urllib.request.urlretrieve(\"https://raw.githubusercontent.com/e9t/nsmc/master/ratings_test.txt\", filename=\"ratings_test.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-ZsMaSqPcUsA"
   },
   "outputs": [],
   "source": [
    "# 데이터셋 로드\n",
    "train_data = pd.read_table('ratings_train.txt')\n",
    "test_data = pd.read_table('ratings_test.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "H7PKdFhpcVFf"
   },
   "outputs": [],
   "source": [
    "# 학습 데이터 확인\n",
    "print ('Train data len :',len(train_data))\n",
    "train_data[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zVmMbF_Cch4_"
   },
   "outputs": [],
   "source": [
    "# 테스트 데이터 확인\n",
    "print ('Test data len :',len(test_data))\n",
    "test_data[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yNEQppqocup6"
   },
   "outputs": [],
   "source": [
    "# 구현 필요\n",
    "# 학습 데이터에서 중복되는 데이터 제거 및 이후 상태 확인 \n",
    "train_data.drop_duplicates(subsets=['document'], inplace=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Lq32ioD4c52j"
   },
   "outputs": [],
   "source": [
    "train_data.groupby('label').size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ig1RRwCGc_46"
   },
   "outputs": [],
   "source": [
    "#구현 필요\n",
    "# 한글 및 공백만 남도록 전처리 및 결과 확인\n",
    "train_data['document'] = train_data['document'].str.replace(\"[ㄱ-ㅎㅏ-ㅣ가-힣 \",\"\") # 한글 및 공백 이외 제거\n",
    "train_data[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ls3_WQwsdd55"
   },
   "outputs": [],
   "source": [
    "#구현 필요\n",
    "# 정규식으로 전처리 후 아예 공백만 남은 문장이 있을 수 있다\n",
    "# -> Null 으로 변경 후 일괄 제거\n",
    "# white space 데이터를 empty value로 변경\n",
    "train_data['document'] = train_data['document'].str.replace('^ +', \"\")\n",
    "train_data['document'].replace('', np.nan, inplace=True)\n",
    "print(train_data.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OIRz3JOCeAds"
   },
   "outputs": [],
   "source": [
    "train_data.loc[train_data.document.isnull()][:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nGZodg_jd0-a"
   },
   "outputs": [],
   "source": [
    "#구현 필요\n",
    "# null값 제거\n",
    "train_data = train_data.dropna(how= 'any')\n",
    "print(len(train_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JjqvZPE9eDoK"
   },
   "outputs": [],
   "source": [
    "stopwords = ['의','가','이','은','들','는','좀','잘','걍','과','도','를','으로','자','에','와','한','하다']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OClOAaCZeG3I"
   },
   "outputs": [],
   "source": [
    "# 불용어 정의 및 형태소 분석기를 이용한 토큰화 진행\n",
    "# stem = True 옵션으로 형태소 분석을 수행할 시 형태소 원형이 반환된다.\n",
    "# 되나요 -> 되다 와 같은 형식으로 반환되는 형태소가 바뀜\n",
    "okt = Okt()\n",
    "X_train = []\n",
    "for sentence in tqdm(train_data['document']):\n",
    "    tokenized_sentence = okt.morphs(sentence, stem=True) # 토큰화\n",
    "    stopwords_removed_sentence = [word for word in tokenized_sentence if not word in stopwords] # 불용어 제거\n",
    "    X_train.append(stopwords_removed_sentence)\n",
    "print(X_train[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lpJOtnfcelyf"
   },
   "outputs": [],
   "source": [
    "# 데스트 데이터에 대해서도 토큰화 진행\n",
    "# 기본적으로 각종 전처리 및 vocab 생성은 학습 데이터에 대해서만 이루어짐\n",
    "# 데스트 데이터는 실제 모델 서비스 시 무엇이 들어올지 모르지 모르는 상태로 가정\n",
    "# 테스트 데이터는 숫자형 데이터도 존재하는 등 데이터 정제가 되지 않은 상태이므로 string으로 캐스팅 후 토큰화 수행\n",
    "X_test = []\n",
    "for sentence in tqdm(test_data['document']):\n",
    "    tokenized_sentence = okt.morphs(str(sentence), stem=True) # 토큰화\n",
    "    stopwords_removed_sentence = [word for word in tokenized_sentence if not word in stopwords] # 불용어 제거\n",
    "    X_test.append(stopwords_removed_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9lMefob9evPH"
   },
   "outputs": [],
   "source": [
    "# 토큰화 된 단어들에 대해서 vocab 생성 및 단어 인덱스 형태의 데이터셋 생성\n",
    "from collections import Counter\n",
    "def tokenize(x_train,y_train,x_val,y_val):\n",
    "    word_list = []\n",
    "\n",
    "    for sent in x_train:\n",
    "      for word in sent:        \n",
    "          word_list.append(word)\n",
    "  \n",
    "    corpus = Counter(word_list)\n",
    "    # sorting on the basis of most common words\n",
    "    corpus_ = sorted(corpus,key=corpus.get,reverse=True)[:10000]\n",
    "    # creating a dict\n",
    "    onehot_dict = {w:i+1 for i,w in enumerate(corpus_)}\n",
    "    \n",
    "    # tokenize\n",
    "    final_list_train,final_list_test = [],[]\n",
    "    for sent in x_train:\n",
    "            final_list_train.append([onehot_dict[word] for word in sent\n",
    "                                     if word in onehot_dict.keys()])\n",
    "    for sent in x_val:\n",
    "            final_list_test.append([onehot_dict[word] for word in sent \n",
    "                                    if word in onehot_dict.keys()])\n",
    "   \n",
    "    return np.array(final_list_train), np.array(y_train),np.array(final_list_test), np.array(y_val),onehot_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "J8PJ17NtgAz9"
   },
   "outputs": [],
   "source": [
    "#구현 필요 \n",
    "# 데이터셋 생성 및 문장 길이(단어 단위)에 대한 통계 분석\n",
    "x_train, y_train, X_test, y_test, vocab = tokenize(X_train, train_data['label'], X_test, test_data['label'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XbqQOfzHi018"
   },
   "outputs": [],
   "source": [
    "# 데이터셋 생성 및 문장 길이(단어 단위)에 대한 통계 분석\n",
    "rev_len = [len(i) for i in x_train]\n",
    "pd.Series(rev_len).hist()\n",
    "plt.show()\n",
    "pd.Series(rev_len).describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XcTDD9sji4fj"
   },
   "outputs": [],
   "source": [
    "# 대부분의 문장을 그대로 담을 수 있는 길이 50으로 데이터 padding 진행\n",
    "def padding_(sentences, seq_len):\n",
    "    features = np.zeros((len(sentences), seq_len),dtype=int)\n",
    "    for ii, review in enumerate(sentences):\n",
    "        if len(review) != 0:\n",
    "            features[ii, -len(review):] = np.array(review)[:seq_len]\n",
    "    return features\n",
    "\n",
    "# 50 확인\n",
    "x_train_pad = padding_(x_train,50)\n",
    "x_test_pad = padding_(x_test,50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HLbIUyqFjCKX"
   },
   "outputs": [],
   "source": [
    "# 데이터셋 생성 (IMDb 실습 코드와 동일)\n",
    "# create Tensor datasets\n",
    "train_data = TensorDataset(torch.from_numpy(x_train_pad), torch.from_numpy(y_train))\n",
    "test_data = TensorDataset(torch.from_numpy(x_test_pad), torch.from_numpy(y_test))\n",
    "\n",
    "# dataloaders\n",
    "batch_size = 50\n",
    "\n",
    "# make sure to SHUFFLE your data\n",
    "train_loader = DataLoader(train_data, shuffle=True, batch_size=batch_size)\n",
    "test_loader = DataLoader(test_data, shuffle=False, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "z86aVye-jNIT"
   },
   "outputs": [],
   "source": [
    "# 모델 정의\n",
    "class GRU_model(nn.Module):\n",
    "    def __init__(self, n_layers, hidden_dim, n_vocab, embed_dim, n_classes, device):\n",
    "        super(GRU_model, self).__init__()\n",
    "        self.n_layers = n_layers\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.device = device\n",
    "\n",
    "        self.embed = nn.Embedding(n_vocab, embed_dim)\n",
    "        self.gru = nn.GRU(embed_dim, self.hidden_dim,\n",
    "                          num_layers=self.n_layers,\n",
    "                          batch_first=True)\n",
    "        self.out = nn.Linear(self.hidden_dim, n_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embed(x)\n",
    "        h_0 = self._init_state(batch_size=x.size(0)) # 첫번째 히든 스테이트를 0벡터로 초기화\n",
    "        x, _ = self.gru(x, h_0)  # GRU의 리턴값은 (배치 크기, 시퀀스 길이, 은닉 상태의 크기)\n",
    "        h_t = x[:,-1,:] # (배치 크기, 은닉 상태의 크기)의 텐서로 크기가 변경됨. 즉, 마지막 time-step의 은닉 상태만 가져온다.\n",
    "        logit = self.out(h_t)  # (배치 크기, 은닉 상태의 크기) -> (배치 크기, 출력층의 크기)\n",
    "        return logit\n",
    "\n",
    "    def _init_state(self, batch_size):\n",
    "        new_state = torch.zeros(self.n_layers, batch_size, self.hidden_dim).to(self.device)\n",
    "        return new_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "q5M1fA5BjYpV"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'torch' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "\u001B[1;32m/Users/gimseong-eon/Downloads/AIP_13_NLP_for_Korean_Texts.ipynb 셀 26\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[0;32m----> <a href='vscode-notebook-cell:/Users/gimseong-eon/Downloads/AIP_13_NLP_for_Korean_Texts.ipynb#X34sZmlsZQ%3D%3D?line=0'>1</a>\u001B[0m is_cuda \u001B[39m=\u001B[39m torch\u001B[39m.\u001B[39mcuda\u001B[39m.\u001B[39mis_available()\n\u001B[1;32m      <a href='vscode-notebook-cell:/Users/gimseong-eon/Downloads/AIP_13_NLP_for_Korean_Texts.ipynb#X34sZmlsZQ%3D%3D?line=2'>3</a>\u001B[0m \u001B[39m# If we have a GPU available, we'll set our device to GPU. We'll use this device variable later in our code.\u001B[39;00m\n\u001B[1;32m      <a href='vscode-notebook-cell:/Users/gimseong-eon/Downloads/AIP_13_NLP_for_Korean_Texts.ipynb#X34sZmlsZQ%3D%3D?line=3'>4</a>\u001B[0m \u001B[39mif\u001B[39;00m is_cuda:\n",
      "\u001B[0;31mNameError\u001B[0m: name 'torch' is not defined"
     ]
    }
   ],
   "source": [
    "# Device 설정 (IMDb 실습 코드와 동일)\n",
    "is_cuda = torch.cuda.is_available()\n",
    "\n",
    "# If we have a GPU available, we'll set our device to GPU. We'll use this device variable later in our code.\n",
    "if is_cuda:\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(\"GPU is available\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"GPU not available, CPU used\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Hznyt7jzjR0r"
   },
   "outputs": [],
   "source": [
    "# 하이퍼파라미터 설정 및 모델 객체 생성\n",
    "n_layers = 1\n",
    "vocab_size = len(vocab) + 1  # extra 1 for <pad>\n",
    "hidden_dim = 128\n",
    "embed_dim = 100\n",
    "n_classes = 2\n",
    "\n",
    "model = GRU_model(n_layers, hidden_dim, vocab_size, embed_dim, n_classes, device).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WebNkEr7jama"
   },
   "outputs": [],
   "source": [
    "# 학습 및 테스트 함수 정의\n",
    "def train(model, criterion, optimizer, data_loader):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    for i, (x, y) in enumerate(data_loader):\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        logit = model(x)\n",
    "        loss = criterion(logit, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item() * x.size(0)\n",
    "      \n",
    "    return train_loss / len(data_loader.dataset)\n",
    "\n",
    "def evaluate(model, data_loader):\n",
    "    model.eval()\n",
    "    corrects, total_loss = 0, 0\n",
    "    for i, (x, y) in enumerate(data_loader):\n",
    "        x, y = x.to(device), y.to(device)\n",
    "\n",
    "        logit = model(x)\n",
    "        corrects += (logit.max(1)[1].view(y.size()).data == y.data).sum()\n",
    "    size = len(data_loader.dataset)\n",
    "    \n",
    "    avg_accuracy = 100.0 * corrects / size\n",
    "    return avg_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NnXPS9S5jeQj"
   },
   "outputs": [],
   "source": [
    "# 학습 및 테스트 loop\n",
    "num_epochs = 10\n",
    "lr = 0.001\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "for e in range(1, num_epochs+1):\n",
    "    train_loss = train(model, criterion, optimizer, train_loader)\n",
    "    test_accuracy = evaluate(model, test_loader)\n",
    "\n",
    "    print(\"[Epoch: %d] train loss : %5.2f | test accuracy : %5.2f\" % (e, train_loss, test_accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1fABWVUepH3W"
   },
   "source": [
    "**BERT 기반 감성분석**"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "**BERT를 활용하기 위해 Transformers 설치 및 데이터셋 로드**"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jHs_7dRWjgNK"
   },
   "outputs": [],
   "source": [
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9CfIiq0tr1GA"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import urllib.request\n",
    "import numpy as np\n",
    "\n",
    "urllib.request.urlretrieve(\"https://raw.githubusercontent.com/e9t/nsmc/master/ratings_train.txt\", filename=\"ratings_train.txt\")\n",
    "urllib.request.urlretrieve(\"https://raw.githubusercontent.com/e9t/nsmc/master/ratings_test.txt\", filename=\"ratings_test.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "5maH92lQr1GC"
   },
   "outputs": [],
   "source": [
    "train_data = pd.read_table('ratings_train.txt')\n",
    "test_data = pd.read_table('ratings_test.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "**테스트 데이터를 전처리 해주지 않으면 토큰화 과정에서 에러가 발생함을 확인하여 학습 데이터와 같은 형식으로 일괄 전처리 진행**\n",
    "**실제 서비스 상황을 가정한다면 테스트 데이터는 매번 입력하기 전에 전처리하여 만약 빈 입력이 되면 스킵하는 것으로 처리**"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uxys4KmduEWm"
   },
   "outputs": [],
   "source": [
    "train_data.drop_duplicates(subset=['document'], inplace=True)  # 중복 데이터 제거\n",
    "train_data['document'] = train_data['document'].str.replace(\"[^ㄱ-ㅎㅏ-ㅣ가-힣 ]\",\"\")  # 한글 및 공백 이외 제거\n",
    "train_data['document'] = train_data['document'].str.replace('^ +', \"\") # white space 데이터를 empty value로 변경\n",
    "train_data['document'].replace('', np.nan, inplace=True)\n",
    "train_data = train_data.dropna(how = 'any')  # null값 제거"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "h7QiJXU0uqOC"
   },
   "outputs": [],
   "source": [
    "test_data['document'] = test_data['document'].str.replace(\"[^ㄱ-ㅎㅏ-ㅣ가-힣 ]\",\"\")  # 한글 및 공백 이외 제거\n",
    "test_data['document'] = test_data['document'].str.replace('^ +', \"\") # white space 데이터를 empty value로 변경\n",
    "test_data['document'].replace('', np.nan, inplace=True)\n",
    "test_data = test_data.dropna(how = 'any')  # null값 제거"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rKpepxcIpcfB"
   },
   "outputs": [],
   "source": [
    "# 한국어 대상 pre-trained BERT 모델이 여러 가지 있으나 klue/bert-base 활용하여 실습 진행\n",
    "# 학습 속도 및 메모리 문제 때문에 데이터셋은 10000개 까지만 샘플링하여 활용\n",
    "\n",
    "\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "#구현 필요\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"klue/bert-base\", num_labels=2)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"klue/bert-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ti6quHP0p1Xg"
   },
   "outputs": [],
   "source": [
    "# BERT는 word piece라는 sub-word 토큰화 알고리즘을 사용하기 때문에 별도의 형태소 분석 없이 바로 토큰화 가능\n",
    "\n",
    "sampled_train_data = train_data[:10000]\n",
    "sampled_test_data = test_data[:10000]\n",
    "\n",
    "#구현 필요\n",
    "train_tokens = tokenizer(list(sampled_train_data['document']), padding='max_length', truncation=True, return_tensors=\"pt\", add_special_tokens=True)\n",
    "test_tokens = tokenizer(list(sampled_test_data['document']), padding='max_length', truncation=True, return_tensors=\"pt\", add_special_tokens=True)\n",
    "print(tokenizer.convert_ids_to_tokens(train_tokens['input_ids'][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tV-hFHsfuypa"
   },
   "outputs": [],
   "source": [
    "#Trainer에 입력해주기 위한 데이터셋 생성\n",
    "import torch\n",
    "\n",
    "class BERTDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: val[idx] for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "#구현 필요\n",
    "train_dataset = BERTDataset(train_tokens, list(sampled_train_data['label']))\n",
    "test_dataset = BERTDataset(test_tokens, list(sampled_test_data['label']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zgITXXp1v9q0"
   },
   "outputs": [],
   "source": [
    "# 학습 환경 설정 및 정확도 확인을 위한 별도 함수 작성\n",
    "from transformers import Trainer, TrainingArguments\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',          # output directory\n",
    "    num_train_epochs=1,              # total number of training epochs\n",
    "    per_device_train_batch_size=10,  # batch size per device during training\n",
    "    per_device_eval_batch_size=20,   # batch size for evaluation\n",
    "    warmup_steps=100,                # number of warmup steps for learning rate scheduler\n",
    "    weight_decay=0.01,               # strength of weight decay\n",
    "    logging_dir='./logs',            # directory for storing logs\n",
    "    logging_steps=250,\n",
    ")\n",
    "\n",
    "def compute_metrics(pred):\n",
    "    labels = pred.label_ids\n",
    "    preds = pred.predictions.argmax(-1)\n",
    "    \n",
    "    acc = accuracy_score(labels, preds)    \n",
    "    return {\n",
    "        'accuracy': acc\n",
    "    }\n",
    "# Trainer 객체 생성 및 학습 진행\n",
    "trainer = Trainer(\n",
    "    model=model,                         # the instantiated 🤗 Transformers model to be trained\n",
    "    args=training_args,                  # training arguments, defined above\n",
    "    train_dataset=train_dataset,         # training dataset\n",
    "    eval_dataset=test_dataset,           # evaluation dataset\n",
    "    compute_metrics=compute_metrics      # additional evaluation metrics\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fMu-lUhhwJQa"
   },
   "outputs": [],
   "source": [
    "trainer.evaluate()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3.9.13 ('GPU_base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "2c7976fc9c27cce57a91a4093c4fd52c90bb73b0f6b779a22b5201159a1e33a5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}