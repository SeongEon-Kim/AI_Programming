{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**RNNì„ ì´ìš©í•œ ë„¤ì´ë²„ ì˜í™” ë¦¬ë·° ê°ì„± ë¶„ì„**\n",
    "//2~7ì€ NLPì—ì„œ í•„ìš”í•œ ê³¼ì •\n",
    "1. ë°ì´í„° ë¡œë“œ\n",
    "2. ì¤‘ë³µ ë°ì´í„° ì œê±°\n",
    "3. ì •ê·œì‹ ì´ìš©í•œ í•œê¸€ ë° ê³µë°± ì´ì™¸ ë¬¸ì ì œê±°\n",
    "4. Null ë°ì´í„° ì œê±°\n",
    "5. í˜•íƒœì†Œ ë¶„ì„ê¸°ë¥¼ í™œìš©í•œ í† í°í™” ë° ë¶ˆìš©ì–´ ì œê±° -> í•œêµ­ì–´ NLP íŠ¹í™”ëœ ì²˜ë¦¬ê°€ í•„ìš”í•œ ê³¼ì •\n",
    "6. Vocab ìƒì„± ë° ë‹¨ì–´ ì¸ë±ìŠ¤ í˜•íƒœì˜ ë°ì´í„°ì…‹ ìƒì„±\n",
    "7. ë°ì´í„°ì— ëŒ€í•œ padding ì§„í–‰\n",
    "8. PyTorch ë°ì´í„°ì…‹ ìƒì„±\n",
    "9. ëª¨ë¸ ì •ì˜, í•™ìŠµ ë° í…ŒìŠ¤íŠ¸ í•¨ìˆ˜ ì •ì˜;\n",
    "10. í•™ìŠµ ë° í…ŒìŠ¤íŠ¸ ì§„í–‰\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "**ìì—°ì–¸ì–´ ì¢…ë¥˜ 3ê°€ì§€**\n",
    "**ê³ ë¦½ì–´: í•œìë¥¼ ì‚¬ìš©í•˜ëŠ” ì¤‘êµ­ì–´ì™€ ê°™ì´ ë‹¨ì–´ì˜ í˜•íƒœê°€ ì‹œì œ, ì¸ì¹­ ë³€í™” ë“±ì— ë”°ë¼ ë°”ë€Œì§€ ì•Šê³  ê³ ì •ëœ í˜•íƒœì˜ ì–¸ì–´**\n",
    "**êµ´ì ˆì–´: ì˜ì–´, ë…ì¼ì–´ ë“±ê³¼ ê°™ì´ ì‹œì œ, ì¸ì¹­ ë²¼í™” ë“±ì— ë”°ë¼ ë‹¨ì–´ì˜ í˜•íƒœê°€ ë³€í•˜ëŠ” ì–¸ì–´ (ë„ì–´ì“°ê¸°ë¡œ splití•´ë„ ë†’ì€ í€„ë¦¬í‹°ì˜ í† í° ìƒì„±)**\n",
    "**êµì°©ì–´: í•œêµ­ì–´, ì¼ë³¸ì–´ ë“±ê³¼ ê°™ì´ ë‹¨ì–´ì— ì–´ë¯¸ê°€ ë¶™ì–´ ì‹œì œ, ì¸ì¹­ ë“±ì˜ ëœ»ì´ ìµœì¢… ê²°ì •ë˜ëŠ” ì–¸ì–´**"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## í•œêµ­ì–´ í…ìŠ¤íŠ¸ í† í°í™”ì˜ ì–´ë ¤ì›€\n",
    "**í•œêµ­ì–´ í…ìŠ¤íŠ¸ëŠ” splitì„ ìˆ˜í–‰í•˜ì—¬ í† í°í™”ë¥¼ í•˜ì˜€ì„ ë•Œ, ì¡°ì‚¬ê°€ ë¶™ì–´ìˆì–´ ì„œë¡œ ë‹¤ë¥¸ ë‹¨ì–´ë¡œ ì¸ì‹**\n",
    "**í•œêµ­ì–´ëŠ” ë„ì–´ì“°ê¸° ë§ì¶¤ë²•ì´ ìˆì§€ë§Œ ë„ì–´ì“°ê¸°ê°€ ì˜ ì§€ì¼œì§€ì§€ ì•ŠëŠ” corpusë„ ë§ìœ¼ë¯€ë¡œ ê¸°ë³¸ì ì¸ splitìœ¼ë¡œ ë§Œì¡±ìŠ¤ëŸ¬ìš´ í† í°ë“¤ì„ ì–»ê¸° ì–´ë ¤ì›€**\n",
    "### -> í•œêµ­ì–´ í…ìŠ¤íŠ¸ì—ì„œ ì˜ì–´ì™€ ìœ ì‚¬í•œ í† í°ë“¤ì„ ì–»ê¸° ìœ„í•´ì„œëŠ” í˜•íƒœì†Œ ë‹¨ìœ„ì˜ ë¶„ë¦¬ê¸° í•„ìš”í•¨"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**í˜•íƒœì†Œ: ì˜ë¯¸ë¥¼ ê°€ì§„ ê°€ì¥ ì‘ì€ ë‹¨ìœ„ì˜ ë§ (ìë¦½ í˜•íƒœì†Œ, ì˜ì¡´ í˜•íƒœì†Œ)**\n",
    "**í˜•íƒœì†Œ ë¶„ì„ê¸°: ì›ë¬¸ í…ìŠ¤íŠ¸ì—ì„œ í˜•íƒœì†Œë¥¼ ì¶”ì¶œí•˜ì—¬ í† í°í™” í•´ì£¼ëŠ” ëª¨ë“ˆ, í”„ë¡œê·¸ë¨**\n",
    "**KoNLPy, ì¹´ì¹´ì˜¤ì˜ khaii ë“±ì´ ìˆìŒ**"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**konlpy.tagì— ëŒ€ì–‘í•œ í˜•íƒœì†Œ ë¶„ì„ê¸°ê°€ ìˆë‹¤. ì‘ë™ ë°©ì‹ê³¼ ì„¸ë¶€ ì„±ëŠ¥ì— ì°¨ì´ê°€ ìˆë‹¤**\n",
    "**ìì£¼ ì“°ì´ëŠ” ë¶„ì„ê¸°ë“¤**\n",
    "**Okt: Open Korean Textì˜ ì¤„ì„ë§ë¡œì¨ ì˜ˆì „ì—ëŠ” Twitter ë¶„ì„ê¸°ë¼ê³  ë¶ˆë ¸ìŒ. \"ã…‹ã…‹\", \"ã„´ã„´\"ë“±ê³¼ ê°™ì€ ì¸í„°ë„· ìš©ì–´ë“¤ë„ ë¹„êµì  ë¶„ì„ì´ ê°€ëŠ¥í•œ ì¥ì ì´ ìˆìŒ**\n",
    "**Kkma: ê¼¬ê¼¬ë§ˆ í˜•íƒ³ì†Œ ë¶„ì„ê¸°ì˜ ì¤„ì„ë§. ì–´ë¯¸ ë¶„ì„ì„ ìì„¸í•˜ê²Œ ìˆ˜í–‰í•´ì¤€ë‹¤ëŠ” ê²ƒì´ ì¥ì **\n",
    "**Mecab: ì¼ë³¸ì–´ í˜•íƒœì†Œ ë¶„ì„ê¸° Mecabì„ í•œêµ­ì–´ ë²„ì „ìœ¼ë¡œ êµ¬í˜„í•œ ê²ƒ(+ê°€ì¥ ë¬´ë‚œí•˜ë‹¤)**"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "!pip install konlpy"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**ìì—°ì–¸ì–´ëŠ” ì„¸ ì¢…ë¥˜ë¡œ êµ¬ë¶„ë¨**\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from konlpy.tag import Okt\n",
    "\n",
    "# KoNLPy í˜•íƒœì†Œ ë¶„ì„ê¸°ë“¤ì€ moprhs ë©”ì„œë“œì— ë¬¸ì¥ì„ ì…ë ¥í•˜ì—¬ í˜•íƒœì†Œ ë¶„ì„ì´ ê°€ëŠ¥\n",
    "# pos ë©”ì„œë“œë¥¼ í†µí•´ í˜•íƒœì†Œ ë¶„ì„ ë° í’ˆì‚¬ íƒœê¹…ì´ ê°€ëŠ¥\n",
    "# í’ˆì‚¬ íƒœê¹…: ê° í˜•íƒœì†Œë§ˆë‹¤ í’ˆì‚¬ë¥¼ ì˜ˆì¸¡í•œ í›„ í˜•íƒœì†Œ-í’ˆì‚¬ pairì„ ë§Œë“œëŠ” ê²ƒ\n",
    "# íŠ¹ì • í’ˆì‚¬ëŠ” ì œê±°í•˜ê³  íŠ¹ì • í’ˆì‚¬ëŠ” ë‚¨ê¸°ëŠ” ë“±ì˜ ì „ì²˜ë¦¬ ê°€ëŠ¥\n",
    "okt = Okt()\n",
    "print('Okt í˜•íƒœì†Œ ë¶„ì„ :', okt.morphs)(\"í•œêµ­ì–´ ë‹¨ì–´ëŠ” í˜•íƒœì†Œë“¤ë¡œ êµ¬ì„±ë˜ì–´ ìˆë‹¤.\")\n",
    "print('Okt í’ˆì‚¬ íƒœê¹… :', okt.pos(\"í•œêµ­ì–´ ë‹¨ì–´ëŠ” í˜•íƒœì†Œë“¤ë¡œ êµ¬ì„±ë˜ì–´ ìˆë‹¤.\"))\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "erNa04VKS35K"
   },
   "outputs": [],
   "source": [
    "from konlpy.tag import Kkma\n",
    "kkma = Kkma()\n",
    "print(\"Kkma í˜•íƒœì†Œ ë¶„ì„ :\", kkma.morphs(\"íŒê·¹ì–´ ë‹¨ì–´ëŠ” í˜•íƒœì†Œë“¤ë¡œ êµ¬ì„±ë˜ì–´ ìˆë‹¤.\"))\n",
    "print('Kkma í’ˆì‚¬ íƒœê¹… :', kkma.pos(\"í•œêµ­ì–´ ë‹¨ì–´ëŠ” í˜•íƒœì†Œë“¤ë¡œ êµ¬ì„±ë˜ì–´ ìˆë‹¤.\"))\n",
    "\n",
    "#êµ¬í˜„ í•„ìš”"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jUacg-RwaK72"
   },
   "source": [
    "**ë„¤ì´ë²„ ì˜í™” ë¦¬ë·° ê°ì„±ë¶„ì„**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "QrXjIS9iYR_g"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'konlpy'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mModuleNotFoundError\u001B[0m                       Traceback (most recent call last)",
      "\u001B[1;32m/Users/gimseong-eon/Downloads/AIP_13_NLP_for_Korean_Texts.ipynb ì…€ 6\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m      <a href='vscode-notebook-cell:/Users/gimseong-eon/Downloads/AIP_13_NLP_for_Korean_Texts.ipynb#W5sZmlsZQ%3D%3D?line=3'>4</a>\u001B[0m \u001B[39mimport\u001B[39;00m \u001B[39mre\u001B[39;00m\n\u001B[1;32m      <a href='vscode-notebook-cell:/Users/gimseong-eon/Downloads/AIP_13_NLP_for_Korean_Texts.ipynb#W5sZmlsZQ%3D%3D?line=4'>5</a>\u001B[0m \u001B[39mimport\u001B[39;00m \u001B[39murllib\u001B[39;00m\u001B[39m.\u001B[39;00m\u001B[39mrequest\u001B[39;00m\n\u001B[0;32m----> <a href='vscode-notebook-cell:/Users/gimseong-eon/Downloads/AIP_13_NLP_for_Korean_Texts.ipynb#W5sZmlsZQ%3D%3D?line=5'>6</a>\u001B[0m \u001B[39mfrom\u001B[39;00m \u001B[39mkonlpy\u001B[39;00m\u001B[39m.\u001B[39;00m\u001B[39mtag\u001B[39;00m \u001B[39mimport\u001B[39;00m Okt\n\u001B[1;32m      <a href='vscode-notebook-cell:/Users/gimseong-eon/Downloads/AIP_13_NLP_for_Korean_Texts.ipynb#W5sZmlsZQ%3D%3D?line=6'>7</a>\u001B[0m \u001B[39mfrom\u001B[39;00m \u001B[39mtqdm\u001B[39;00m \u001B[39mimport\u001B[39;00m tqdm\n\u001B[1;32m      <a href='vscode-notebook-cell:/Users/gimseong-eon/Downloads/AIP_13_NLP_for_Korean_Texts.ipynb#W5sZmlsZQ%3D%3D?line=7'>8</a>\u001B[0m \u001B[39mimport\u001B[39;00m \u001B[39mtorch\u001B[39;00m\n",
      "\u001B[0;31mModuleNotFoundError\u001B[0m: No module named 'konlpy'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import urllib.request\n",
    "from konlpy.tag import Okt\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from collections import Counter\n",
    "#device = torch.device('mps:0' if torch.backends.mps.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_2SWYgPFcSAD"
   },
   "outputs": [],
   "source": [
    "urllib.request.urlretrieve(\"https://raw.githubusercontent.com/e9t/nsmc/master/ratings_train.txt\", filename=\"ratings_train.txt\")\n",
    "urllib.request.urlretrieve(\"https://raw.githubusercontent.com/e9t/nsmc/master/ratings_test.txt\", filename=\"ratings_test.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-ZsMaSqPcUsA"
   },
   "outputs": [],
   "source": [
    "# ë°ì´í„°ì…‹ ë¡œë“œ\n",
    "train_data = pd.read_table('ratings_train.txt')\n",
    "test_data = pd.read_table('ratings_test.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "H7PKdFhpcVFf"
   },
   "outputs": [],
   "source": [
    "# í•™ìŠµ ë°ì´í„° í™•ì¸\n",
    "print ('Train data len :',len(train_data))\n",
    "train_data[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zVmMbF_Cch4_"
   },
   "outputs": [],
   "source": [
    "# í…ŒìŠ¤íŠ¸ ë°ì´í„° í™•ì¸\n",
    "print ('Test data len :',len(test_data))\n",
    "test_data[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yNEQppqocup6"
   },
   "outputs": [],
   "source": [
    "# êµ¬í˜„ í•„ìš”\n",
    "# í•™ìŠµ ë°ì´í„°ì—ì„œ ì¤‘ë³µë˜ëŠ” ë°ì´í„° ì œê±° ë° ì´í›„ ìƒíƒœ í™•ì¸ \n",
    "train_data.drop_duplicates(subsets=['document'], inplace=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Lq32ioD4c52j"
   },
   "outputs": [],
   "source": [
    "train_data.groupby('label').size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ig1RRwCGc_46"
   },
   "outputs": [],
   "source": [
    "#êµ¬í˜„ í•„ìš”\n",
    "# í•œê¸€ ë° ê³µë°±ë§Œ ë‚¨ë„ë¡ ì „ì²˜ë¦¬ ë° ê²°ê³¼ í™•ì¸\n",
    "train_data['document'] = train_data['document'].str.replace(\"[ã„±-ã…ã…-ã…£ê°€-í£ \",\"\") # í•œê¸€ ë° ê³µë°± ì´ì™¸ ì œê±°\n",
    "train_data[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ls3_WQwsdd55"
   },
   "outputs": [],
   "source": [
    "#êµ¬í˜„ í•„ìš”\n",
    "# ì •ê·œì‹ìœ¼ë¡œ ì „ì²˜ë¦¬ í›„ ì•„ì˜ˆ ê³µë°±ë§Œ ë‚¨ì€ ë¬¸ì¥ì´ ìˆì„ ìˆ˜ ìˆë‹¤\n",
    "# -> Null ìœ¼ë¡œ ë³€ê²½ í›„ ì¼ê´„ ì œê±°\n",
    "# white space ë°ì´í„°ë¥¼ empty valueë¡œ ë³€ê²½\n",
    "train_data['document'] = train_data['document'].str.replace('^ +', \"\")\n",
    "train_data['document'].replace('', np.nan, inplace=True)\n",
    "print(train_data.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OIRz3JOCeAds"
   },
   "outputs": [],
   "source": [
    "train_data.loc[train_data.document.isnull()][:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nGZodg_jd0-a"
   },
   "outputs": [],
   "source": [
    "#êµ¬í˜„ í•„ìš”\n",
    "# nullê°’ ì œê±°\n",
    "train_data = train_data.dropna(how= 'any')\n",
    "print(len(train_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JjqvZPE9eDoK"
   },
   "outputs": [],
   "source": [
    "stopwords = ['ì˜','ê°€','ì´','ì€','ë“¤','ëŠ”','ì¢€','ì˜','ê±','ê³¼','ë„','ë¥¼','ìœ¼ë¡œ','ì','ì—','ì™€','í•œ','í•˜ë‹¤']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OClOAaCZeG3I"
   },
   "outputs": [],
   "source": [
    "# ë¶ˆìš©ì–´ ì •ì˜ ë° í˜•íƒœì†Œ ë¶„ì„ê¸°ë¥¼ ì´ìš©í•œ í† í°í™” ì§„í–‰\n",
    "# stem = True ì˜µì…˜ìœ¼ë¡œ í˜•íƒœì†Œ ë¶„ì„ì„ ìˆ˜í–‰í•  ì‹œ í˜•íƒœì†Œ ì›í˜•ì´ ë°˜í™˜ëœë‹¤.\n",
    "# ë˜ë‚˜ìš” -> ë˜ë‹¤ ì™€ ê°™ì€ í˜•ì‹ìœ¼ë¡œ ë°˜í™˜ë˜ëŠ” í˜•íƒœì†Œê°€ ë°”ë€œ\n",
    "okt = Okt()\n",
    "X_train = []\n",
    "for sentence in tqdm(train_data['document']):\n",
    "    tokenized_sentence = okt.morphs(sentence, stem=True) # í† í°í™”\n",
    "    stopwords_removed_sentence = [word for word in tokenized_sentence if not word in stopwords] # ë¶ˆìš©ì–´ ì œê±°\n",
    "    X_train.append(stopwords_removed_sentence)\n",
    "print(X_train[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lpJOtnfcelyf"
   },
   "outputs": [],
   "source": [
    "# ë°ìŠ¤íŠ¸ ë°ì´í„°ì— ëŒ€í•´ì„œë„ í† í°í™” ì§„í–‰\n",
    "# ê¸°ë³¸ì ìœ¼ë¡œ ê°ì¢… ì „ì²˜ë¦¬ ë° vocab ìƒì„±ì€ í•™ìŠµ ë°ì´í„°ì— ëŒ€í•´ì„œë§Œ ì´ë£¨ì–´ì§\n",
    "# ë°ìŠ¤íŠ¸ ë°ì´í„°ëŠ” ì‹¤ì œ ëª¨ë¸ ì„œë¹„ìŠ¤ ì‹œ ë¬´ì—‡ì´ ë“¤ì–´ì˜¬ì§€ ëª¨ë¥´ì§€ ëª¨ë¥´ëŠ” ìƒíƒœë¡œ ê°€ì •\n",
    "# í…ŒìŠ¤íŠ¸ ë°ì´í„°ëŠ” ìˆ«ìí˜• ë°ì´í„°ë„ ì¡´ì¬í•˜ëŠ” ë“± ë°ì´í„° ì •ì œê°€ ë˜ì§€ ì•Šì€ ìƒíƒœì´ë¯€ë¡œ stringìœ¼ë¡œ ìºìŠ¤íŒ… í›„ í† í°í™” ìˆ˜í–‰\n",
    "X_test = []\n",
    "for sentence in tqdm(test_data['document']):\n",
    "    tokenized_sentence = okt.morphs(str(sentence), stem=True) # í† í°í™”\n",
    "    stopwords_removed_sentence = [word for word in tokenized_sentence if not word in stopwords] # ë¶ˆìš©ì–´ ì œê±°\n",
    "    X_test.append(stopwords_removed_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9lMefob9evPH"
   },
   "outputs": [],
   "source": [
    "# í† í°í™” ëœ ë‹¨ì–´ë“¤ì— ëŒ€í•´ì„œ vocab ìƒì„± ë° ë‹¨ì–´ ì¸ë±ìŠ¤ í˜•íƒœì˜ ë°ì´í„°ì…‹ ìƒì„±\n",
    "from collections import Counter\n",
    "def tokenize(x_train,y_train,x_val,y_val):\n",
    "    word_list = []\n",
    "\n",
    "    for sent in x_train:\n",
    "      for word in sent:        \n",
    "          word_list.append(word)\n",
    "  \n",
    "    corpus = Counter(word_list)\n",
    "    # sorting on the basis of most common words\n",
    "    corpus_ = sorted(corpus,key=corpus.get,reverse=True)[:10000]\n",
    "    # creating a dict\n",
    "    onehot_dict = {w:i+1 for i,w in enumerate(corpus_)}\n",
    "    \n",
    "    # tokenize\n",
    "    final_list_train,final_list_test = [],[]\n",
    "    for sent in x_train:\n",
    "            final_list_train.append([onehot_dict[word] for word in sent\n",
    "                                     if word in onehot_dict.keys()])\n",
    "    for sent in x_val:\n",
    "            final_list_test.append([onehot_dict[word] for word in sent \n",
    "                                    if word in onehot_dict.keys()])\n",
    "   \n",
    "    return np.array(final_list_train), np.array(y_train),np.array(final_list_test), np.array(y_val),onehot_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "J8PJ17NtgAz9"
   },
   "outputs": [],
   "source": [
    "#êµ¬í˜„ í•„ìš” \n",
    "# ë°ì´í„°ì…‹ ìƒì„± ë° ë¬¸ì¥ ê¸¸ì´(ë‹¨ì–´ ë‹¨ìœ„)ì— ëŒ€í•œ í†µê³„ ë¶„ì„\n",
    "x_train, y_train, X_test, y_test, vocab = tokenize(X_train, train_data['label'], X_test, test_data['label'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XbqQOfzHi018"
   },
   "outputs": [],
   "source": [
    "# ë°ì´í„°ì…‹ ìƒì„± ë° ë¬¸ì¥ ê¸¸ì´(ë‹¨ì–´ ë‹¨ìœ„)ì— ëŒ€í•œ í†µê³„ ë¶„ì„\n",
    "rev_len = [len(i) for i in x_train]\n",
    "pd.Series(rev_len).hist()\n",
    "plt.show()\n",
    "pd.Series(rev_len).describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XcTDD9sji4fj"
   },
   "outputs": [],
   "source": [
    "# ëŒ€ë¶€ë¶„ì˜ ë¬¸ì¥ì„ ê·¸ëŒ€ë¡œ ë‹´ì„ ìˆ˜ ìˆëŠ” ê¸¸ì´ 50ìœ¼ë¡œ ë°ì´í„° padding ì§„í–‰\n",
    "def padding_(sentences, seq_len):\n",
    "    features = np.zeros((len(sentences), seq_len),dtype=int)\n",
    "    for ii, review in enumerate(sentences):\n",
    "        if len(review) != 0:\n",
    "            features[ii, -len(review):] = np.array(review)[:seq_len]\n",
    "    return features\n",
    "\n",
    "# 50 í™•ì¸\n",
    "x_train_pad = padding_(x_train,50)\n",
    "x_test_pad = padding_(x_test,50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HLbIUyqFjCKX"
   },
   "outputs": [],
   "source": [
    "# ë°ì´í„°ì…‹ ìƒì„± (IMDb ì‹¤ìŠµ ì½”ë“œì™€ ë™ì¼)\n",
    "# create Tensor datasets\n",
    "train_data = TensorDataset(torch.from_numpy(x_train_pad), torch.from_numpy(y_train))\n",
    "test_data = TensorDataset(torch.from_numpy(x_test_pad), torch.from_numpy(y_test))\n",
    "\n",
    "# dataloaders\n",
    "batch_size = 50\n",
    "\n",
    "# make sure to SHUFFLE your data\n",
    "train_loader = DataLoader(train_data, shuffle=True, batch_size=batch_size)\n",
    "test_loader = DataLoader(test_data, shuffle=False, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "z86aVye-jNIT"
   },
   "outputs": [],
   "source": [
    "# ëª¨ë¸ ì •ì˜\n",
    "class GRU_model(nn.Module):\n",
    "    def __init__(self, n_layers, hidden_dim, n_vocab, embed_dim, n_classes, device):\n",
    "        super(GRU_model, self).__init__()\n",
    "        self.n_layers = n_layers\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.device = device\n",
    "\n",
    "        self.embed = nn.Embedding(n_vocab, embed_dim)\n",
    "        self.gru = nn.GRU(embed_dim, self.hidden_dim,\n",
    "                          num_layers=self.n_layers,\n",
    "                          batch_first=True)\n",
    "        self.out = nn.Linear(self.hidden_dim, n_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embed(x)\n",
    "        h_0 = self._init_state(batch_size=x.size(0)) # ì²«ë²ˆì§¸ íˆë“  ìŠ¤í…Œì´íŠ¸ë¥¼ 0ë²¡í„°ë¡œ ì´ˆê¸°í™”\n",
    "        x, _ = self.gru(x, h_0)  # GRUì˜ ë¦¬í„´ê°’ì€ (ë°°ì¹˜ í¬ê¸°, ì‹œí€€ìŠ¤ ê¸¸ì´, ì€ë‹‰ ìƒíƒœì˜ í¬ê¸°)\n",
    "        h_t = x[:,-1,:] # (ë°°ì¹˜ í¬ê¸°, ì€ë‹‰ ìƒíƒœì˜ í¬ê¸°)ì˜ í…ì„œë¡œ í¬ê¸°ê°€ ë³€ê²½ë¨. ì¦‰, ë§ˆì§€ë§‰ time-stepì˜ ì€ë‹‰ ìƒíƒœë§Œ ê°€ì ¸ì˜¨ë‹¤.\n",
    "        logit = self.out(h_t)  # (ë°°ì¹˜ í¬ê¸°, ì€ë‹‰ ìƒíƒœì˜ í¬ê¸°) -> (ë°°ì¹˜ í¬ê¸°, ì¶œë ¥ì¸µì˜ í¬ê¸°)\n",
    "        return logit\n",
    "\n",
    "    def _init_state(self, batch_size):\n",
    "        new_state = torch.zeros(self.n_layers, batch_size, self.hidden_dim).to(self.device)\n",
    "        return new_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "q5M1fA5BjYpV"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'torch' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "\u001B[1;32m/Users/gimseong-eon/Downloads/AIP_13_NLP_for_Korean_Texts.ipynb ì…€ 26\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[0;32m----> <a href='vscode-notebook-cell:/Users/gimseong-eon/Downloads/AIP_13_NLP_for_Korean_Texts.ipynb#X34sZmlsZQ%3D%3D?line=0'>1</a>\u001B[0m is_cuda \u001B[39m=\u001B[39m torch\u001B[39m.\u001B[39mcuda\u001B[39m.\u001B[39mis_available()\n\u001B[1;32m      <a href='vscode-notebook-cell:/Users/gimseong-eon/Downloads/AIP_13_NLP_for_Korean_Texts.ipynb#X34sZmlsZQ%3D%3D?line=2'>3</a>\u001B[0m \u001B[39m# If we have a GPU available, we'll set our device to GPU. We'll use this device variable later in our code.\u001B[39;00m\n\u001B[1;32m      <a href='vscode-notebook-cell:/Users/gimseong-eon/Downloads/AIP_13_NLP_for_Korean_Texts.ipynb#X34sZmlsZQ%3D%3D?line=3'>4</a>\u001B[0m \u001B[39mif\u001B[39;00m is_cuda:\n",
      "\u001B[0;31mNameError\u001B[0m: name 'torch' is not defined"
     ]
    }
   ],
   "source": [
    "# Device ì„¤ì • (IMDb ì‹¤ìŠµ ì½”ë“œì™€ ë™ì¼)\n",
    "is_cuda = torch.cuda.is_available()\n",
    "\n",
    "# If we have a GPU available, we'll set our device to GPU. We'll use this device variable later in our code.\n",
    "if is_cuda:\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(\"GPU is available\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"GPU not available, CPU used\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Hznyt7jzjR0r"
   },
   "outputs": [],
   "source": [
    "# í•˜ì´í¼íŒŒë¼ë¯¸í„° ì„¤ì • ë° ëª¨ë¸ ê°ì²´ ìƒì„±\n",
    "n_layers = 1\n",
    "vocab_size = len(vocab) + 1  # extra 1 for <pad>\n",
    "hidden_dim = 128\n",
    "embed_dim = 100\n",
    "n_classes = 2\n",
    "\n",
    "model = GRU_model(n_layers, hidden_dim, vocab_size, embed_dim, n_classes, device).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WebNkEr7jama"
   },
   "outputs": [],
   "source": [
    "# í•™ìŠµ ë° í…ŒìŠ¤íŠ¸ í•¨ìˆ˜ ì •ì˜\n",
    "def train(model, criterion, optimizer, data_loader):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    for i, (x, y) in enumerate(data_loader):\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        logit = model(x)\n",
    "        loss = criterion(logit, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item() * x.size(0)\n",
    "      \n",
    "    return train_loss / len(data_loader.dataset)\n",
    "\n",
    "def evaluate(model, data_loader):\n",
    "    model.eval()\n",
    "    corrects, total_loss = 0, 0\n",
    "    for i, (x, y) in enumerate(data_loader):\n",
    "        x, y = x.to(device), y.to(device)\n",
    "\n",
    "        logit = model(x)\n",
    "        corrects += (logit.max(1)[1].view(y.size()).data == y.data).sum()\n",
    "    size = len(data_loader.dataset)\n",
    "    \n",
    "    avg_accuracy = 100.0 * corrects / size\n",
    "    return avg_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NnXPS9S5jeQj"
   },
   "outputs": [],
   "source": [
    "# í•™ìŠµ ë° í…ŒìŠ¤íŠ¸ loop\n",
    "num_epochs = 10\n",
    "lr = 0.001\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "for e in range(1, num_epochs+1):\n",
    "    train_loss = train(model, criterion, optimizer, train_loader)\n",
    "    test_accuracy = evaluate(model, test_loader)\n",
    "\n",
    "    print(\"[Epoch: %d] train loss : %5.2f | test accuracy : %5.2f\" % (e, train_loss, test_accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1fABWVUepH3W"
   },
   "source": [
    "**BERT ê¸°ë°˜ ê°ì„±ë¶„ì„**"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "**BERTë¥¼ í™œìš©í•˜ê¸° ìœ„í•´ Transformers ì„¤ì¹˜ ë° ë°ì´í„°ì…‹ ë¡œë“œ**"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jHs_7dRWjgNK"
   },
   "outputs": [],
   "source": [
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9CfIiq0tr1GA"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import urllib.request\n",
    "import numpy as np\n",
    "\n",
    "urllib.request.urlretrieve(\"https://raw.githubusercontent.com/e9t/nsmc/master/ratings_train.txt\", filename=\"ratings_train.txt\")\n",
    "urllib.request.urlretrieve(\"https://raw.githubusercontent.com/e9t/nsmc/master/ratings_test.txt\", filename=\"ratings_test.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "5maH92lQr1GC"
   },
   "outputs": [],
   "source": [
    "train_data = pd.read_table('ratings_train.txt')\n",
    "test_data = pd.read_table('ratings_test.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "**í…ŒìŠ¤íŠ¸ ë°ì´í„°ë¥¼ ì „ì²˜ë¦¬ í•´ì£¼ì§€ ì•Šìœ¼ë©´ í† í°í™” ê³¼ì •ì—ì„œ ì—ëŸ¬ê°€ ë°œìƒí•¨ì„ í™•ì¸í•˜ì—¬ í•™ìŠµ ë°ì´í„°ì™€ ê°™ì€ í˜•ì‹ìœ¼ë¡œ ì¼ê´„ ì „ì²˜ë¦¬ ì§„í–‰**\n",
    "**ì‹¤ì œ ì„œë¹„ìŠ¤ ìƒí™©ì„ ê°€ì •í•œë‹¤ë©´ í…ŒìŠ¤íŠ¸ ë°ì´í„°ëŠ” ë§¤ë²ˆ ì…ë ¥í•˜ê¸° ì „ì— ì „ì²˜ë¦¬í•˜ì—¬ ë§Œì•½ ë¹ˆ ì…ë ¥ì´ ë˜ë©´ ìŠ¤í‚µí•˜ëŠ” ê²ƒìœ¼ë¡œ ì²˜ë¦¬**"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uxys4KmduEWm"
   },
   "outputs": [],
   "source": [
    "train_data.drop_duplicates(subset=['document'], inplace=True)  # ì¤‘ë³µ ë°ì´í„° ì œê±°\n",
    "train_data['document'] = train_data['document'].str.replace(\"[^ã„±-ã…ã…-ã…£ê°€-í£ ]\",\"\")  # í•œê¸€ ë° ê³µë°± ì´ì™¸ ì œê±°\n",
    "train_data['document'] = train_data['document'].str.replace('^ +', \"\") # white space ë°ì´í„°ë¥¼ empty valueë¡œ ë³€ê²½\n",
    "train_data['document'].replace('', np.nan, inplace=True)\n",
    "train_data = train_data.dropna(how = 'any')  # nullê°’ ì œê±°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "h7QiJXU0uqOC"
   },
   "outputs": [],
   "source": [
    "test_data['document'] = test_data['document'].str.replace(\"[^ã„±-ã…ã…-ã…£ê°€-í£ ]\",\"\")  # í•œê¸€ ë° ê³µë°± ì´ì™¸ ì œê±°\n",
    "test_data['document'] = test_data['document'].str.replace('^ +', \"\") # white space ë°ì´í„°ë¥¼ empty valueë¡œ ë³€ê²½\n",
    "test_data['document'].replace('', np.nan, inplace=True)\n",
    "test_data = test_data.dropna(how = 'any')  # nullê°’ ì œê±°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rKpepxcIpcfB"
   },
   "outputs": [],
   "source": [
    "# í•œêµ­ì–´ ëŒ€ìƒ pre-trained BERT ëª¨ë¸ì´ ì—¬ëŸ¬ ê°€ì§€ ìˆìœ¼ë‚˜ klue/bert-base í™œìš©í•˜ì—¬ ì‹¤ìŠµ ì§„í–‰\n",
    "# í•™ìŠµ ì†ë„ ë° ë©”ëª¨ë¦¬ ë¬¸ì œ ë•Œë¬¸ì— ë°ì´í„°ì…‹ì€ 10000ê°œ ê¹Œì§€ë§Œ ìƒ˜í”Œë§í•˜ì—¬ í™œìš©\n",
    "\n",
    "\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "#êµ¬í˜„ í•„ìš”\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"klue/bert-base\", num_labels=2)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"klue/bert-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ti6quHP0p1Xg"
   },
   "outputs": [],
   "source": [
    "# BERTëŠ” word pieceë¼ëŠ” sub-word í† í°í™” ì•Œê³ ë¦¬ì¦˜ì„ ì‚¬ìš©í•˜ê¸° ë•Œë¬¸ì— ë³„ë„ì˜ í˜•íƒœì†Œ ë¶„ì„ ì—†ì´ ë°”ë¡œ í† í°í™” ê°€ëŠ¥\n",
    "\n",
    "sampled_train_data = train_data[:10000]\n",
    "sampled_test_data = test_data[:10000]\n",
    "\n",
    "#êµ¬í˜„ í•„ìš”\n",
    "train_tokens = tokenizer(list(sampled_train_data['document']), padding='max_length', truncation=True, return_tensors=\"pt\", add_special_tokens=True)\n",
    "test_tokens = tokenizer(list(sampled_test_data['document']), padding='max_length', truncation=True, return_tensors=\"pt\", add_special_tokens=True)\n",
    "print(tokenizer.convert_ids_to_tokens(train_tokens['input_ids'][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tV-hFHsfuypa"
   },
   "outputs": [],
   "source": [
    "#Trainerì— ì…ë ¥í•´ì£¼ê¸° ìœ„í•œ ë°ì´í„°ì…‹ ìƒì„±\n",
    "import torch\n",
    "\n",
    "class BERTDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: val[idx] for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "#êµ¬í˜„ í•„ìš”\n",
    "train_dataset = BERTDataset(train_tokens, list(sampled_train_data['label']))\n",
    "test_dataset = BERTDataset(test_tokens, list(sampled_test_data['label']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zgITXXp1v9q0"
   },
   "outputs": [],
   "source": [
    "# í•™ìŠµ í™˜ê²½ ì„¤ì • ë° ì •í™•ë„ í™•ì¸ì„ ìœ„í•œ ë³„ë„ í•¨ìˆ˜ ì‘ì„±\n",
    "from transformers import Trainer, TrainingArguments\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',          # output directory\n",
    "    num_train_epochs=1,              # total number of training epochs\n",
    "    per_device_train_batch_size=10,  # batch size per device during training\n",
    "    per_device_eval_batch_size=20,   # batch size for evaluation\n",
    "    warmup_steps=100,                # number of warmup steps for learning rate scheduler\n",
    "    weight_decay=0.01,               # strength of weight decay\n",
    "    logging_dir='./logs',            # directory for storing logs\n",
    "    logging_steps=250,\n",
    ")\n",
    "\n",
    "def compute_metrics(pred):\n",
    "    labels = pred.label_ids\n",
    "    preds = pred.predictions.argmax(-1)\n",
    "    \n",
    "    acc = accuracy_score(labels, preds)    \n",
    "    return {\n",
    "        'accuracy': acc\n",
    "    }\n",
    "# Trainer ê°ì²´ ìƒì„± ë° í•™ìŠµ ì§„í–‰\n",
    "trainer = Trainer(\n",
    "    model=model,                         # the instantiated ğŸ¤— Transformers model to be trained\n",
    "    args=training_args,                  # training arguments, defined above\n",
    "    train_dataset=train_dataset,         # training dataset\n",
    "    eval_dataset=test_dataset,           # evaluation dataset\n",
    "    compute_metrics=compute_metrics      # additional evaluation metrics\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fMu-lUhhwJQa"
   },
   "outputs": [],
   "source": [
    "trainer.evaluate()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3.9.13 ('GPU_base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "2c7976fc9c27cce57a91a4093c4fd52c90bb73b0f6b779a22b5201159a1e33a5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}